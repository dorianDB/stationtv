# √âvaluation de la Qualit√© sans Texte de R√©f√©rence - Station TV

## üéØ Le Probl√®me

**WER classique** n√©cessite un texte de r√©f√©rence (v√©rit√© terrain) :
```
WER = Erreurs / Nombre de mots de r√©f√©rence
```

‚ùå Pas de r√©f√©rence humaine disponible pour 588h d'audio

---

## ‚úÖ Solutions Alternatives

### 1. √âchantillonnage Manuel (Recommand√©)

**M√©thode :**
- S√©lectionner **10-20 √©chantillons** de 1-2 minutes chacun
- Transcrire **manuellement** ces courts extraits (r√©f√©rence)
- Comparer avec transcriptions Whisper
- Calculer le WER sur ces √©chantillons

**Avantages :**
- ‚úÖ WER pr√©cis sur √©chantillon repr√©sentatif
- ‚úÖ ~2-3h de travail manuel pour 20 √©chantillons
- ‚úÖ Permet de comparer Small vs Medium

**Script fourni :**
```python
# Utiliser qos/metrics.py
from qos.metrics import MetricsCalculator

calc = MetricsCalculator()
wer = calc.calculate_wer(
    reference_text="texte manuel",
    hypothesis_text="transcription whisper"
)
print(f"WER: {wer*100:.2f}%")
```

---

### 2. Comparaison Small vs Medium (WER Relatif)

**M√©thode :**
- Transcrire le **m√™me fichier** avec Small ET Medium
- Comparer les deux transcriptions
- Les diff√©rences indiquent o√π Medium am√©liore

**Avantages :**
- ‚úÖ Pas de travail manuel
- ‚úÖ Identifie les zones d'am√©lioration
- ‚ö†Ô∏è Ne donne pas un WER absolu

**Pour votre fichier test :**
```
Small  : fichier_transcript_ws.txt
Medium : fichier_transcript_wm.txt
```

Vous pouvez les comparer visuellement ou avec un script de diff.

---

### 3. Scores de Confiance Whisper

**M√©thode :**
- Whisper fournit un **score de confiance** pour chaque segment
- Les segments avec confiance < 0.6 sont suspects
- R√©viser manuellement ces segments

**Impl√©mentation :**

Le syst√®me peut √™tre modifi√© pour extraire ces scores :

```python
# Dans core/transcription.py, modifier transcribe_on_specific_cores()
result = model.transcribe(audio_path, language=self.language)

# Acc√©der aux scores de confiance
for segment in result["segments"]:
    if segment.get("confidence", 1.0) < 0.6:
        print(f"‚ö†Ô∏è Segment suspect: {segment['text']}")
```

**Avantages :**
- ‚úÖ Automatique
- ‚úÖ Identifie zones probl√©matiques
- ‚ö†Ô∏è Ne donne pas un WER pr√©cis

---

### 4. M√©triques Linguistiques Automatiques

**Perplexit√©** : Mesure la "fluidit√©" linguistique
- Texte fluide = bon
- Texte incoh√©rent = erreurs probables

**Coh√©rence s√©mantique** : 
- V√©rifier la coh√©rence du contexte
- D√©tecter phrases sans sens

**Outils disponibles :**
- `language-tool-python` (corrections grammaticales)
- `spacy` (analyse linguistique)

---

### 5. Validation Crois√©e (Cross-Model)

**M√©thode :**
- Utiliser 2-3 mod√®les diff√©rents (small, medium, large)
- Comparer leurs transcriptions
- Consensus = probable correct
- Divergence = zone d'erreur possible

**Exemple :**
```
Small  : "le chat mange"
Medium : "le chat mange"  ‚úÖ Consensus
Large  : "le chat mange"

vs

Small  : "il va √† la maison"
Medium : "il va √† la mairie"  ‚ö†Ô∏è Divergence
Large  : "il va √† la mairie"  ‚Üí Probable = "mairie"
```

---

## üéØ Recommandation pour Station TV

### Approche Hybride (Optimal)

**√âtape 1 - √âchantillonnage (1 fois)**
```
1. S√©lectionner 20 extraits de 1 min (vari√©t√© de cha√Ænes/√©missions)
2. Transcrire manuellement (2-3h de travail)
3. Calculer WER r√©el sur ces √©chantillons
4. Benchmarker Small vs Medium
```

**R√©sultats attendus :**
- WER Small : ~8-12% (segments clairs)
- WER Medium : ~5-8% (segments clairs)
- WER sur audio bruit√© : +5-10%

**√âtape 2 - Production (Quotidien)**
```
1. Utiliser scores de confiance Whisper
2. Flaguer segments < 0.6 de confiance
3. R√©vision manuelle spot-check (5% du contenu)
```

---

## üìä WER Attendus (Litt√©rature)

Bas√© sur les benchmarks Whisper OpenAI :

| Mod√®le | WER (Audio Propre) | WER (Audio Bruit√©) |
|--------|-------------------|-------------------|
| **Tiny** | 15-20% | 25-35% |
| **Small** | 8-12% | 15-22% |
| **Medium** | 5-8% | 10-15% |
| **Large** | 3-5% | 7-12% |

**Pour TNT fran√ßaise :**
- Qualit√© audio : G√©n√©ralement bonne (studio)
- WER attendu Medium : **~6-8%** ‚úÖ
- WER attendu Small : **~10-12%**

---

## üí° Solution Pratique Imm√©diate

### Script de Comparaison Small vs Medium

Je peux cr√©er un script qui :
1. Lit les deux transcriptions (small et medium)
2. Compare mot √† mot
3. Calcule un "WER relatif"
4. G√©n√®re un rapport HTML avec diff√©rences color√©es

**Voulez-vous que je cr√©e ce script ?**

---

## üìù Synth√®se

**Sans texte de r√©f√©rence :**

‚úÖ **Faisable** : √âchantillonnage manuel (20 extraits √ó 1 min)
- Effort : 2-3h de travail
- R√©sultat : WER pr√©cis sur √©chantillon repr√©sentatif

‚úÖ **Automatique** : Comparaison Small vs Medium
- Effort : 0 (d√©j√† fait)
- R√©sultat : Diff√©rences qualitatives

‚úÖ **Continu** : Scores de confiance Whisper
- Effort : Int√©grer dans le code
- R√©sultat : D√©tection automatique des zones suspectes

**Ma recommandation** : Commencer par comparer visuellement vos deux transcriptions existantes (small vs medium) pour voir la diff√©rence de qualit√©, puis faire un √©chantillonnage manuel si vous voulez un WER pr√©cis.
